{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtHF88M8SXZk"
   },
   "source": [
    "# 기계 번역(Neural Machine Translation)\n",
    "## [1] 영한 번역기 구현 :   Character-Level 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2398,
     "status": "ok",
     "timestamp": 1600594597442,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "XWn6aM4tSXZm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3933,
     "status": "ok",
     "timestamp": 1600594598982,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "VkxtFa8sSXZz"
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/kor-eng.zip'\n",
    "filename = 'kor-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3921,
     "status": "ok",
     "timestamp": 1600594598983,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Gk8Sy8bYSXZ7",
    "outputId": "f6629fc7-b909-422f-cb01-7d7b3677139f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3638, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines= pd.read_csv('kor.txt', names=['src', 'tar'], sep='\\t', index_col =False)\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3911,
     "status": "ok",
     "timestamp": 1600594598984,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "RidzraTvSXaj",
    "outputId": "43e39760-c3f0-483b-9883-72d0b697a7b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>가.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>안녕.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>뛰어!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run.</td>\n",
       "      <td>뛰어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>우와!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>쏴!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Help!</td>\n",
       "      <td>도와줘!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jump!</td>\n",
       "      <td>점프!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>점프해.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>기다려!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>잠깐!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>기다려.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Begin.</td>\n",
       "      <td>시작해.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>안녕!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       src   tar\n",
       "0      Go.    가.\n",
       "1      Hi.   안녕.\n",
       "2     Run!   뛰어!\n",
       "3     Run.   뛰어.\n",
       "4     Who?   누구?\n",
       "5     Wow!   우와!\n",
       "6    Fire!    쏴!\n",
       "7    Help!  도와줘!\n",
       "8    Jump!   점프!\n",
       "9    Jump.  점프해.\n",
       "10   Wait!  기다려!\n",
       "11   Wait!   잠깐!\n",
       "12   Wait.  기다려.\n",
       "13  Begin.  시작해.\n",
       "14  Hello!   안녕!"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3903,
     "status": "ok",
     "timestamp": 1600594598985,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "mYTJqAO8SXaw",
    "outputId": "32519d9d-a866-49c7-e4db-04607b06fef8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Somebody was talking to Tom.</td>\n",
       "      <td>\\t 누군가 톰한테 말을 걸고 있었어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>Someone ate all my cupcakes.</td>\n",
       "      <td>\\t 누가 내 컵케익 다 먹었어 \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>That's why I can't help you.</td>\n",
       "      <td>\\t 이게 바로 내가 널 도와줄 수 없는 이유야. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>The cat is still very young.</td>\n",
       "      <td>\\t 그 고양이는 아직도 많이 어려. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>There's a full moon tonight.</td>\n",
       "      <td>\\t 오늘밤은 보름달이 떴어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>I should've bought more food.</td>\n",
       "      <td>\\t 내가 먹을 것을 더 샀어야 했는데. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>I think that Tom is offended.</td>\n",
       "      <td>\\t 내 생각에 톰은 기분이 상해있어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>I think that Tom is sensible.</td>\n",
       "      <td>\\t 톰은 예민한 것 같아. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>I tried to act appropriately.</td>\n",
       "      <td>\\t 난 적절하게 처신하려고 노력했어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>I used to be a heroin addict.</td>\n",
       "      <td>\\t 나는 한때 헤로인 중독자였어. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                src                             tar\n",
       "2000   Somebody was talking to Tom.        \\t 누군가 톰한테 말을 걸고 있었어. \\n\n",
       "2001   Someone ate all my cupcakes.            \\t 누가 내 컵케익 다 먹었어 \\n\n",
       "2002   That's why I can't help you.  \\t 이게 바로 내가 널 도와줄 수 없는 이유야. \\n\n",
       "2003   The cat is still very young.         \\t 그 고양이는 아직도 많이 어려. \\n\n",
       "2004   There's a full moon tonight.             \\t 오늘밤은 보름달이 떴어. \\n\n",
       "...                             ...                             ...\n",
       "2095  I should've bought more food.       \\t 내가 먹을 것을 더 샀어야 했는데. \\n\n",
       "2096  I think that Tom is offended.        \\t 내 생각에 톰은 기분이 상해있어. \\n\n",
       "2097  I think that Tom is sensible.              \\t 톰은 예민한 것 같아. \\n\n",
       "2098  I tried to act appropriately.        \\t 난 적절하게 처신하려고 노력했어. \\n\n",
       "2099  I used to be a heroin addict.          \\t 나는 한때 헤로인 중독자였어. \\n\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target 컬럼 'tar'에 '\\t'와 '\\n'을 앞 뒤로 추가\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines[2000:2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3900,
     "status": "ok",
     "timestamp": 1600594598985,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "LpVBxjtiSXbj"
   },
   "outputs": [],
   "source": [
    "# 'src'와 'tar'의 글자 집합 구축\n",
    "src_vocab=set()\n",
    "for line in lines.src:   # 1줄씩 읽음\n",
    "    for char in line:    # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "# print(src_vocab)  # 영어의 글자 수\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)\n",
    "# print(tar_vocab)   # 한국어의 글자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3891,
     "status": "ok",
     "timestamp": 1600594598986,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "B0heErOdSXb8",
    "outputId": "4a75b495-4aa5-46ab-dfc5-c53a3c7fb5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "912\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3880,
     "status": "ok",
     "timestamp": 1600594598987,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "1wurtoiWSXcA",
    "outputId": "8a4c79b8-4ca6-4a79-a689-e5cdf54b0d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'ï']\n",
      "['간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '개', '객', '갰', '걀', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '게', '겐', '겠', '겨', '격', '겪', '견']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3869,
     "status": "ok",
     "timestamp": 1600594598988,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Hm8XLW_WSXcD",
    "outputId": "a3c40ff8-f21e-4e4a-8b2f-8fba80474db4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'Y': 46, 'a': 47, 'b': 48, 'c': 49, 'd': 50, 'e': 51, 'f': 52, 'g': 53, 'h': 54, 'i': 55, 'j': 56, 'k': 57, 'l': 58, 'm': 59, 'n': 60, 'o': 61, 'p': 62, 'q': 63, 'r': 64, 's': 65, 't': 66, 'u': 67, 'v': 68, 'w': 69, 'x': 70, 'y': 71, 'z': 72, '°': 73, 'ï': 74}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '%': 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'H': 29, 'M': 30, 'N': 31, 'T': 32, 'a': 33, 'd': 34, 'h': 35, 'i': 36, 'm': 37, 'o': 38, 'p': 39, 'r': 40, 't': 41, 'y': 42, '°': 43, '가': 44, '각': 45, '간': 46, '갇': 47, '갈': 48, '감': 49, '갑': 50, '값': 51, '갔': 52, '강': 53, '갖': 54, '같': 55, '개': 56, '객': 57, '갰': 58, '걀': 59, '걔': 60, '거': 61, '걱': 62, '건': 63, '걷': 64, '걸': 65, '검': 66, '겁': 67, '것': 68, '게': 69, '겐': 70, '겠': 71, '겨': 72, '격': 73, '겪': 74, '견': 75, '결': 76, '겼': 77, '경': 78, '계': 79, '고': 80, '곡': 81, '곤': 82, '곧': 83, '골': 84, '곰': 85, '곱': 86, '곳': 87, '공': 88, '과': 89, '관': 90, '광': 91, '괜': 92, '괴': 93, '굉': 94, '교': 95, '구': 96, '국': 97, '군': 98, '굳': 99, '굴': 100, '굶': 101, '굼': 102, '굽': 103, '궁': 104, '권': 105, '귀': 106, '귄': 107, '규': 108, '그': 109, '극': 110, '근': 111, '글': 112, '금': 113, '급': 114, '긋': 115, '긍': 116, '기': 117, '긴': 118, '길': 119, '깊': 120, '까': 121, '깎': 122, '깐': 123, '깔': 124, '깜': 125, '깡': 126, '깨': 127, '꺼': 128, '꺾': 129, '껍': 130, '껏': 131, '껐': 132, '께': 133, '껴': 134, '꼈': 135, '꼬': 136, '꼴': 137, '꼼': 138, '꽃': 139, '꽉': 140, '꽤': 141, '꾸': 142, '꾼': 143, '꿇': 144, '꿈': 145, '꿔': 146, '꿨': 147, '뀌': 148, '끄': 149, '끈': 150, '끊': 151, '끌': 152, '끓': 153, '끔': 154, '끗': 155, '끙': 156, '끝': 157, '끼': 158, '낀': 159, '낄': 160, '낌': 161, '나': 162, '낙': 163, '낚': 164, '난': 165, '날': 166, '낡': 167, '남': 168, '납': 169, '났': 170, '낭': 171, '낮': 172, '낯': 173, '내': 174, '낸': 175, '낼': 176, '냄': 177, '냈': 178, '냉': 179, '냐': 180, '냥': 181, '너': 182, '넌': 183, '널': 184, '넓': 185, '넘': 186, '넛': 187, '넣': 188, '네': 189, '넷': 190, '녀': 191, '녁': 192, '년': 193, '념': 194, '녕': 195, '노': 196, '녹': 197, '논': 198, '놀': 199, '농': 200, '높': 201, '놓': 202, '놔': 203, '놨': 204, '뇌': 205, '누': 206, '눅': 207, '눈': 208, '눠': 209, '눴': 210, '뉴': 211, '늄': 212, '느': 213, '늑': 214, '는': 215, '늘': 216, '늙': 217, '능': 218, '늦': 219, '니': 220, '닌': 221, '님': 222, '다': 223, '닥': 224, '닦': 225, '단': 226, '닫': 227, '달': 228, '닮': 229, '담': 230, '답': 231, '당': 232, '대': 233, '댔': 234, '더': 235, '덕': 236, '던': 237, '덜': 238, '덤': 239, '덥': 240, '데': 241, '덴': 242, '도': 243, '독': 244, '돈': 245, '돌': 246, '돕': 247, '동': 248, '돼': 249, '됐': 250, '되': 251, '된': 252, '될': 253, '됩': 254, '두': 255, '둑': 256, '둔': 257, '둘': 258, '둠': 259, '둬': 260, '뒀': 261, '뒤': 262, '드': 263, '득': 264, '든': 265, '듣': 266, '들': 267, '듯': 268, '등': 269, '디': 270, '딘': 271, '딨': 272, '따': 273, '딱': 274, '딸': 275, '땅': 276, '땋': 277, '때': 278, '떠': 279, '떡': 280, '떤': 281, '떨': 282, '떴': 283, '떻': 284, '또': 285, '똑': 286, '뚱': 287, '뛰': 288, '뜨': 289, '뜰': 290, '뜻': 291, '라': 292, '락': 293, '란': 294, '랄': 295, '람': 296, '랍': 297, '랐': 298, '랑': 299, '래': 300, '랜': 301, '램': 302, '랩': 303, '랬': 304, '략': 305, '량': 306, '러': 307, '럭': 308, '런': 309, '럴': 310, '럼': 311, '럽': 312, '렀': 313, '렁': 314, '렇': 315, '레': 316, '렌': 317, '렛': 318, '려': 319, '력': 320, '련': 321, '렵': 322, '렸': 323, '령': 324, '례': 325, '로': 326, '록': 327, '론': 328, '롭': 329, '뢰': 330, '료': 331, '루': 332, '룹': 333, '류': 334, '륙': 335, '륜': 336, '륭': 337, '르': 338, '른': 339, '를': 340, '름': 341, '릅': 342, '릎': 343, '리': 344, '린': 345, '릴': 346, '림': 347, '립': 348, '마': 349, '막': 350, '만': 351, '많': 352, '말': 353, '맙': 354, '맛': 355, '망': 356, '맞': 357, '맡': 358, '매': 359, '맥': 360, '맨': 361, '맷': 362, '머': 363, '먹': 364, '먼': 365, '멀': 366, '멈': 367, '멋': 368, '멍': 369, '메': 370, '멕': 371, '멜': 372, '며': 373, '면': 374, '명': 375, '몇': 376, '모': 377, '목': 378, '몰': 379, '몸': 380, '못': 381, '묘': 382, '무': 383, '묵': 384, '묶': 385, '문': 386, '묻': 387, '물': 388, '뭇': 389, '뭐': 390, '뭔': 391, '뭘': 392, '므': 393, '미': 394, '민': 395, '믿': 396, '밀': 397, '밌': 398, '밍': 399, '밑': 400, '바': 401, '박': 402, '밖': 403, '반': 404, '받': 405, '발': 406, '밝': 407, '밟': 408, '밤': 409, '밥': 410, '방': 411, '배': 412, '백': 413, '뱀': 414, '버': 415, '벅': 416, '번': 417, '벌': 418, '범': 419, '법': 420, '벗': 421, '벙': 422, '베': 423, '벼': 424, '벽': 425, '변': 426, '별': 427, '병': 428, '보': 429, '복': 430, '본': 431, '볼': 432, '봄': 433, '봅': 434, '봇': 435, '봉': 436, '봐': 437, '봤': 438, '부': 439, '북': 440, '분': 441, '불': 442, '붉': 443, '붐': 444, '붕': 445, '붙': 446, '브': 447, '블': 448, '비': 449, '빈': 450, '빌': 451, '빙': 452, '빛': 453, '빠': 454, '빨': 455, '빴': 456, '빵': 457, '빼': 458, '뺄': 459, '뻔': 460, '뻤': 461, '뽀': 462, '뽑': 463, '뿌': 464, '뿐': 465, '쁘': 466, '쁜': 467, '쁠': 468, '삐': 469, '사': 470, '삭': 471, '산': 472, '살': 473, '삶': 474, '삼': 475, '샀': 476, '상': 477, '새': 478, '색': 479, '샌': 480, '생': 481, '샤': 482, '샴': 483, '서': 484, '석': 485, '선': 486, '설': 487, '섬': 488, '섭': 489, '섯': 490, '섰': 491, '성': 492, '세': 493, '센': 494, '셈': 495, '셔': 496, '셜': 497, '셨': 498, '소': 499, '속': 500, '손': 501, '솔': 502, '송': 503, '쇠': 504, '수': 505, '숙': 506, '순': 507, '숟': 508, '술': 509, '숨': 510, '쉬': 511, '쉽': 512, '슈': 513, '스': 514, '슨': 515, '슬': 516, '습': 517, '승': 518, '시': 519, '식': 520, '신': 521, '실': 522, '싫': 523, '심': 524, '십': 525, '싱': 526, '싶': 527, '싸': 528, '쌉': 529, '써': 530, '썩': 531, '썼': 532, '썽': 533, '쏘': 534, '쏠': 535, '쏴': 536, '쓰': 537, '쓱': 538, '쓴': 539, '쓸': 540, '씀': 541, '씨': 542, '씩': 543, '씬': 544, '씻': 545, '아': 546, '악': 547, '안': 548, '앉': 549, '않': 550, '알': 551, '앓': 552, '암': 553, '압': 554, '앗': 555, '았': 556, '앙': 557, '앞': 558, '애': 559, '액': 560, '앨': 561, '앵': 562, '야': 563, '약': 564, '얀': 565, '얇': 566, '양': 567, '얗': 568, '얘': 569, '어': 570, '억': 571, '언': 572, '얻': 573, '얼': 574, '엄': 575, '업': 576, '없': 577, '엇': 578, '었': 579, '엌': 580, '에': 581, '엔': 582, '엘': 583, '여': 584, '역': 585, '연': 586, '열': 587, '염': 588, '엽': 589, '였': 590, '영': 591, '옆': 592, '예': 593, '옛': 594, '오': 595, '옥': 596, '온': 597, '올': 598, '옮': 599, '옳': 600, '옷': 601, '옹': 602, '와': 603, '완': 604, '왔': 605, '왜': 606, '외': 607, '왼': 608, '요': 609, '욕': 610, '용': 611, '우': 612, '운': 613, '울': 614, '움': 615, '웃': 616, '워': 617, '원': 618, '월': 619, '웠': 620, '웨': 621, '위': 622, '윈': 623, '윗': 624, '윙': 625, '유': 626, '육': 627, '윤': 628, '으': 629, '은': 630, '을': 631, '음': 632, '읍': 633, '응': 634, '의': 635, '이': 636, '익': 637, '인': 638, '일': 639, '읽': 640, '잃': 641, '임': 642, '입': 643, '있': 644, '잊': 645, '자': 646, '작': 647, '잔': 648, '잖': 649, '잘': 650, '잠': 651, '잡': 652, '잤': 653, '장': 654, '재': 655, '잭': 656, '쟁': 657, '저': 658, '적': 659, '전': 660, '절': 661, '젊': 662, '점': 663, '접': 664, '정': 665, '제': 666, '젝': 667, '젠': 668, '젯': 669, '져': 670, '졌': 671, '조': 672, '족': 673, '존': 674, '졸': 675, '좀': 676, '종': 677, '좋': 678, '좌': 679, '죄': 680, '죠': 681, '주': 682, '죽': 683, '준': 684, '줄': 685, '중': 686, '줘': 687, '줬': 688, '쥐': 689, '즈': 690, '즉': 691, '즐': 692, '즘': 693, '증': 694, '지': 695, '직': 696, '진': 697, '질': 698, '집': 699, '짓': 700, '짖': 701, '짜': 702, '짝': 703, '짧': 704, '째': 705, '쨌': 706, '쩔': 707, '쩡': 708, '쪄': 709, '쪘': 710, '쪼': 711, '쪽': 712, '쫓': 713, '쯤': 714, '찌': 715, '찍': 716, '찔': 717, '찡': 718, '찢': 719, '차': 720, '착': 721, '찬': 722, '찮': 723, '찰': 724, '참': 725, '찼': 726, '창': 727, '찾': 728, '채': 729, '책': 730, '챌': 731, '챘': 732, '처': 733, '척': 734, '천': 735, '철': 736, '첫': 737, '청': 738, '체': 739, '쳐': 740, '쳤': 741, '초': 742, '촌': 743, '총': 744, '최': 745, '추': 746, '축': 747, '출': 748, '춤': 749, '충': 750, '춰': 751, '췄': 752, '취': 753, '츠': 754, '측': 755, '치': 756, '칙': 757, '친': 758, '칠': 759, '침': 760, '칩': 761, '카': 762, '캐': 763, '커': 764, '컨': 765, '컴': 766, '컵': 767, '케': 768, '켓': 769, '켜': 770, '켤': 771, '켰': 772, '코': 773, '콜': 774, '콥': 775, '콩': 776, '쾅': 777, '쿠': 778, '쿨': 779, '퀴': 780, '큐': 781, '크': 782, '큰': 783, '큼': 784, '키': 785, '킬': 786, '킹': 787, '타': 788, '탁': 789, '탄': 790, '탈': 791, '탐': 792, '탑': 793, '탓': 794, '탔': 795, '탕': 796, '태': 797, '택': 798, '터': 799, '턱': 800, '턴': 801, '테': 802, '텐': 803, '토': 804, '톰': 805, '톱': 806, '통': 807, '퇴': 808, '투': 809, '트': 810, '특': 811, '튼': 812, '틀': 813, '티': 814, '틱': 815, '틴': 816, '팀': 817, '팅': 818, '파': 819, '판': 820, '팔': 821, '팠': 822, '패': 823, '퍼': 824, '펐': 825, '페': 826, '펙': 827, '펜': 828, '펭': 829, '펴': 830, '편': 831, '평': 832, '폐': 833, '포': 834, '폭': 835, '폰': 836, '표': 837, '푸': 838, '푹': 839, '풀': 840, '품': 841, '풍': 842, '퓨': 843, '프': 844, '픈': 845, '플': 846, '픔': 847, '피': 848, '필': 849, '핑': 850, '하': 851, '학': 852, '한': 853, '할': 854, '함': 855, '합': 856, '항': 857, '해': 858, '핸': 859, '햄': 860, '했': 861, '행': 862, '향': 863, '햿': 864, '허': 865, '헉': 866, '헌': 867, '험': 868, '헤': 869, '헬': 870, '헷': 871, '혀': 872, '현': 873, '혈': 874, '혐': 875, '협': 876, '혔': 877, '형': 878, '혜': 879, '호': 880, '혹': 881, '혼': 882, '홉': 883, '홋': 884, '화': 885, '확': 886, '환': 887, '활': 888, '황': 889, '회': 890, '획': 891, '효': 892, '후': 893, '훈': 894, '훌': 895, '훔': 896, '훨': 897, '휘': 898, '휴': 899, '흐': 900, '흔': 901, '흘': 902, '흙': 903, '흠': 904, '흡': 905, '흥': 906, '희': 907, '흰': 908, '히': 909, '힌': 910, '힘': 911}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3855,
     "status": "ok",
     "timestamp": 1600594598989,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Q6lgAf0DSXcf",
    "outputId": "7b98b09b-d712-44c2-9621-5c7932f44887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29, 61, 9], [30, 55, 9], [40, 67, 60, 2], [40, 67, 60, 9], [45, 54, 61, 22]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src: # 입력 영어 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line:     #각 줄에서 1개씩 글자를 읽음\n",
    "        temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])  # 5개 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3837,
     "status": "ok",
     "timestamp": 1600594598990,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "nhKrM0eaSXc_",
    "outputId": "20083d45-0019-4809-f9cd-5c3a71fdf145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 44, 11, 3, 2], [1, 3, 548, 195, 11, 3, 2], [1, 3, 288, 570, 4, 3, 2], [1, 3, 288, 570, 11, 3, 2], [1, 3, 206, 96, 24, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar: # 출력 한국어 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])  # 5개 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3823,
     "status": "ok",
     "timestamp": 1600594598990,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "toKvk4Y9SXdR",
    "outputId": "94633ed5-c8a2-4850-97ff-e4584a5c92d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 44, 11, 3, 2], [3, 548, 195, 11, 3, 2], [3, 288, 570, 4, 3, 2], [3, 288, 570, 11, 3, 2], [3, 206, 96, 24, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 'tar' 한국어 문장의 맨앞의 '\\t'(인코딩 값:1)를 모두 제거한다 \n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3810,
     "status": "ok",
     "timestamp": 1600594598991,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "qzItvuJWSXdY",
    "outputId": "35830f5d-cb79-4187-a949-c551ec32c6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# 영어와 한국어 문장의 최대 길이를 구한다\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3798,
     "status": "ok",
     "timestamp": 1600594598992,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "cncNsU8PSXdc",
    "outputId": "29aedd7f-457f-459a-f0b0-a996bfe3d4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이를 10으로 나누어 사용 (학습 시간 단축)\n",
    "max_src_len //= 10\n",
    "max_tar_len //= 10\n",
    "print(max_src_len)  # 53\n",
    "print(max_tar_len)  # 30  \n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4380,
     "status": "ok",
     "timestamp": 1600594599576,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "vBrEcyhuSXdp"
   },
   "outputs": [],
   "source": [
    "# 원핫 벡터\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xVsjwZiSXdu"
   },
   "source": [
    "### seq2seq 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4378,
     "status": "ok",
     "timestamp": 1600594599576,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "h_nYF-iMSXdv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5295,
     "status": "ok",
     "timestamp": 1600594600495,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "lbXAvKhxSXd1"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개 : 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5283,
     "status": "ok",
     "timestamp": 1600594600496,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "pPO8-Ho2SXd7",
    "outputId": "de8ba2b0-eb66-4749-cab7-e450096e9757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 75)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 912)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 339968      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  1197056     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 912)    234384      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,771,408\n",
      "Trainable params: 1,771,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()\n",
    "\n",
    "# 글자  레벨이므로  Encoder 측에 워드임베딩을 위한 Embedding 계층을 사용하지 않았음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66429,
     "status": "ok",
     "timestamp": 1600594661653,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Plp5hX84SXd_",
    "outputId": "156c5a84-1b58-4b8c-a6de-53c98489ca91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 2.8188 - val_loss: 4.3874\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 2.3469 - val_loss: 4.3298\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 2.2371 - val_loss: 4.2990\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 2.1152 - val_loss: 3.8069\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.9595 - val_loss: 3.3972\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.7817 - val_loss: 3.3498\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.6798 - val_loss: 3.4196\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.6262 - val_loss: 3.3394\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.5621 - val_loss: 3.2771\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.5155 - val_loss: 3.2639\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.4884 - val_loss: 3.2226\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.4467 - val_loss: 3.1715\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.4097 - val_loss: 3.1376\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.3994 - val_loss: 3.1063\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.3433 - val_loss: 3.1453\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.3128 - val_loss: 3.0172\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.2818 - val_loss: 2.9884\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.2813 - val_loss: 3.1672\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.2383 - val_loss: 2.9639\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.2119 - val_loss: 2.9247\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.1869 - val_loss: 3.0673\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.1616 - val_loss: 2.9950\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.1384 - val_loss: 2.9225\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.1158 - val_loss: 3.0491\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.0930 - val_loss: 2.9525\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.0707 - val_loss: 2.9953\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.0640 - val_loss: 2.9529\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.0280 - val_loss: 2.9423\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.0110 - val_loss: 2.9230\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.9890 - val_loss: 2.8770\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.9688 - val_loss: 3.0537\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.9490 - val_loss: 2.8716\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.9287 - val_loss: 2.8466\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.9109 - val_loss: 2.8648\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.8938 - val_loss: 2.7651\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.8718 - val_loss: 2.8540\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.8528 - val_loss: 2.8396\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.8315 - val_loss: 2.7889\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.8122 - val_loss: 2.8461\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.7972 - val_loss: 2.8768\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.7791 - val_loss: 2.9543\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.7577 - val_loss: 3.0134\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.7414 - val_loss: 2.8432\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.7230 - val_loss: 2.8375\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.7052 - val_loss: 3.0214\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.6910 - val_loss: 2.8778\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.6730 - val_loss: 2.9711\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.6587 - val_loss: 3.1570\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.6445 - val_loss: 2.9343\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.6286 - val_loss: 2.9584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7716695d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66418,
     "status": "ok",
     "timestamp": 1600594661654,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "cnyz3JU_SXeE",
    "outputId": "8c3cb886-94a0-41b6-f514-050aecb4f4b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 75)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 339968    \n",
      "=================================================================\n",
      "Total params: 339,968\n",
      "Trainable params: 339,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66407,
     "status": "ok",
     "timestamp": 1600594661655,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "U_FlFWOYSXeJ",
    "outputId": "b4624c6c-906c-4deb-ae0d-b0bb587f86cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 912)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  1197056     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 912)    234384      lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,431,440\n",
      "Trainable params: 1,431,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66403,
     "status": "ok",
     "timestamp": 1600594661655,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "lvQcWFG3SXeN"
   },
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66402,
     "status": "ok",
     "timestamp": 1600594661657,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "sGw6UlqzSXeQ"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 73018,
     "status": "ok",
     "timestamp": 1600594668285,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "7hD5leU5SXeU",
    "outputId": "f462ab04-7a01-4116-ff88-0e1aa80c4bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Help me!\n",
      "정답 문장:  도와줘! \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: Eat slowly.\n",
      "정답 문장:  천천히 먹어. \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: Mama cried.\n",
      "정답 문장:  엄마는 울었어. \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: Look there.\n",
      "정답 문장:  저기를 봐. \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: Please sit.\n",
      "정답 문장:  앉아 줘. \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: He smiled.\n",
      "정답 문장:  그 사람은 미소지었어. \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: I won!\n",
      "정답 문장:  내가 이겼어! \n",
      "번역기가 번역한 문장:  난 톰이 거짓말 하지 않았어. \n",
      "-----------------------------------\n",
      "입력 문장: How awful!\n",
      "정답 문장:  끔찍해라! \n",
      "번역기가 번역한 문장:  그 사람들은 거미를 무서해서서 왔어. \n",
      "-----------------------------------\n",
      "입력 문장: Tom drove.\n",
      "정답 문장:  톰이 운전했어. \n",
      "번역기가 번역한 문장:  톰은 아직도 꽤 어려. \n",
      "-----------------------------------\n",
      "입력 문장: That hurts.\n",
      "정답 문장:  아파. \n",
      "번역기가 번역한 문장:  이 사람들은 의학 부를 다고 있어. \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(10): # 입력 문장의 인덱스\n",
    "    seq_index = random.randint(10,300)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfxtCWA1SXeb"
   },
   "source": [
    "## [2]  프랑스어-영어 번역기 구현 : Word-Level 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 73017,
     "status": "ok",
     "timestamp": 1600594668286,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "mVXArZmISXec"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "duytxDKvSXen"
   },
   "source": [
    "### 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75285,
     "status": "ok",
     "timestamp": 1600594670557,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "VlGszwDySXes"
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75284,
     "status": "ok",
     "timestamp": 1600594670559,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "jClHm6__SXe9"
   },
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fnywYhLSXfC"
   },
   "source": [
    "### 전처리 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75283,
     "status": "ok",
     "timestamp": 1600594670560,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "djhQLDurSXfD"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75282,
     "status": "ok",
     "timestamp": 1600594670561,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "j9XqiT_cSXfH"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 위에서 구현한 함수를 내부적으로 호출\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75272,
     "status": "ok",
     "timestamp": 1600594670562,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "aAiEDvMuSXfL",
    "outputId": "94fc81ea-4ed4-403b-a46b-0a00bf55f0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you had dinner ?\n",
      "b'avez vous deja dine ?'\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(preprocess_sentence(en_sent))\n",
    "print(preprocess_sentence(fr_sent).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75271,
     "status": "ok",
     "timestamp": 1600594670563,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Dbl0tj8zSXfU"
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"fra.txt\", \"r\", encoding='utf-8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line_input = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line_input)\n",
    "            decoder_input.append(tar_line_input)\n",
    "            decoder_target.append(tar_line_target)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78761,
     "status": "ok",
     "timestamp": 1600594674063,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Bd7jDPPkSXfY",
    "outputId": "0e6658dc-c5ee-48ee-c57f-3d1a92b6a9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['hi', '.'], ['hi', '.'], ['run', '!'], ['run', '!']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.'], ['<sos>', 'cours', '!'], ['<sos>', 'courez', '!']]\n",
      "[['va', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>'], ['cours', '!', '<eos>'], ['courez', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print(sents_en_in[:5])\n",
    "print(sents_fra_in[:5])\n",
    "print(sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Hxz5pUSXfi"
   },
   "source": [
    "#### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78759,
     "status": "ok",
     "timestamp": 1600594674064,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "es3dTrL1SXfq"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBhfzlH1SXfu"
   },
   "source": [
    "#### 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78758,
     "status": "ok",
     "timestamp": 1600594674065,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "jP0D9FZySXfv"
   },
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78747,
     "status": "ok",
     "timestamp": 1600594674065,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Z3kyuPD-SXf1",
    "outputId": "26c6550d-444e-4da3-b3a6-16012215c47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4663, 프랑스어 단어 집합의 크기 : 8038\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78743,
     "status": "ok",
     "timestamp": 1600594674065,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "KWzfxQbYSXf8"
   },
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\n",
    "\n",
    "tar_to_index = tokenizer_fra.word_index # 훈련 후 예측 과정에서 사용\n",
    "index_to_tar = tokenizer_fra.index_word # 훈련 후 결과 비교할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78734,
     "status": "ok",
     "timestamp": 1600594674066,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Wl9N3ftKSXf_",
    "outputId": "530a2eea-f20e-4e98-f118-33e40a2beaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28578 12218  8432 ...  2978 24205 10096]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78732,
     "status": "ok",
     "timestamp": 1600594674066,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "MscryFXZSXgC"
   },
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78725,
     "status": "ok",
     "timestamp": 1600594674067,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "iuoXjYvBSXgH",
    "outputId": "f1d73ce6-5b4e-4dfc-d201-c781e7932b83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5,    7, 1366,  153,    1,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78716,
     "status": "ok",
     "timestamp": 1600594674067,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "ylXNo1iPSXgM",
    "outputId": "154df9bd-daf2-4a2c-bfbd-5ab47b3e4a2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   19,    5, 2297,  167,  168,    1,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79381,
     "status": "ok",
     "timestamp": 1600594674741,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "Zon8h9h9SXgQ",
    "outputId": "90d0e095-3689-47db-b6fd-b730f103261b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  19,    5, 2297,  167,  168,    1,    3,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79373,
     "status": "ok",
     "timestamp": 1600594674742,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "tQGEva6bSXgU",
    "outputId": "827852dc-d499-4f5d-a15a-9efcc823ff85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print(n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79370,
     "status": "ok",
     "timestamp": 1600594674742,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "-ARpbCn4SXgd"
   },
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79363,
     "status": "ok",
     "timestamp": 1600594674743,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "st2rgSm9SXgr",
    "outputId": "553d9099-d67f-4208-9e41-2c2b1f94bfe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29700, 8)\n",
      "(29700, 16)\n",
      "(29700, 16)\n",
      "(3300, 8)\n",
      "(3300, 16)\n",
      "(3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1BdjcAaSXg3"
   },
   "source": [
    "### 기계 번역기 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79361,
     "status": "ok",
     "timestamp": 1600594674744,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "o2WxuRICSXg4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79359,
     "status": "ok",
     "timestamp": 1600594674744,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "FYOGyGqMSXg_"
   },
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79357,
     "status": "ok",
     "timestamp": 1600594674744,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "YAEZxvTuSXhW"
   },
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79356,
     "status": "ok",
     "timestamp": 1600594674745,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "ihOOi1DYSXhm"
   },
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79355,
     "status": "ok",
     "timestamp": 1600594674745,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "PutYscpySXiF"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79354,
     "status": "ok",
     "timestamp": 1600594674746,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "by8ylCE5SXiR"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79345,
     "status": "ok",
     "timestamp": 1600594674746,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "UQ99SOjvSXiZ",
    "outputId": "5f593703-b12a-4d6b-c11e-c18b6573a0d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     233150      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     401900      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 50)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 50)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50), (None,  20200       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       masking_1[0][0]                  \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8038)   409938      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,085,388\n",
      "Trainable params: 1,085,388\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210410,
     "status": "ok",
     "timestamp": 1600594805819,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "V6ND8EyXSXic",
    "outputId": "296af1dc-0fb7-4690-c1ee-11e8e84cc66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 3.1632 - acc: 0.6079 - val_loss: 1.9248 - val_acc: 0.6554\n",
      "Epoch 2/10\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.7344 - acc: 0.7258 - val_loss: 1.6256 - val_acc: 0.7406\n",
      "Epoch 3/10\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.5464 - acc: 0.7483 - val_loss: 1.5149 - val_acc: 0.7544\n",
      "Epoch 4/10\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.4412 - acc: 0.7669 - val_loss: 1.4270 - val_acc: 0.7730\n",
      "Epoch 5/10\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.3541 - acc: 0.7820 - val_loss: 1.3624 - val_acc: 0.7838\n",
      "Epoch 6/10\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.2863 - acc: 0.7913 - val_loss: 1.3032 - val_acc: 0.7908\n",
      "Epoch 7/10\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.2317 - acc: 0.7990 - val_loss: 1.2634 - val_acc: 0.7981\n",
      "Epoch 8/10\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 1.1879 - acc: 0.8053 - val_loss: 1.2251 - val_acc: 0.8043\n",
      "Epoch 9/10\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 1.1508 - acc: 0.8107 - val_loss: 1.1982 - val_acc: 0.8085\n",
      "Epoch 10/10\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 1.1175 - acc: 0.8155 - val_loss: 1.2007 - val_acc: 0.8078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f768c7b94e0>"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 128, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210414,
     "status": "ok",
     "timestamp": 1600594805825,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "BRFitd6SSXio",
    "outputId": "3cbb662e-5bc8-45f0-e747-695f5588f1d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          233150    \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 253,350\n",
      "Trainable params: 253,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더 : Train과 동일\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210860,
     "status": "ok",
     "timestamp": 1600594806273,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "LrpZfgBLSXiq"
   },
   "outputs": [],
   "source": [
    "# 디코더\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210853,
     "status": "ok",
     "timestamp": 1600594806274,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "qjGeNSQXSXiy",
    "outputId": "afff2b45-4711-4df7-f2d8-428c863cbb1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     401900      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       embedding_1[1][0]                \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8038)   409938      lstm_3[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 832,038\n",
      "Trainable params: 832,038\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210851,
     "status": "ok",
     "timestamp": 1600594806274,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "97dC1yUXSXjM"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210849,
     "status": "ok",
     "timestamp": 1600594806275,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "kGsOWsQqSXje"
   },
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_src[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
    "            temp = temp + index_to_tar[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 214617,
     "status": "ok",
     "timestamp": 1600594810049,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "rkIGwcv9SXjr",
    "outputId": "c7b03fe7-e249-47e6-faf1-023d360f23f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  they canceled . \n",
      "번역문 : elles ont annule . \n",
      "예측문 :  ils sont toutes . \n",
      "\n",
      "\n",
      "원문 :  i lit the candle . \n",
      "번역문 : j allumai le cierge . \n",
      "예측문 :  j ai ete ete . \n",
      "\n",
      "\n",
      "원문 :  we re all at risk . \n",
      "번역문 : nous sommes toutes en danger . \n",
      "예측문 :  nous sommes tous tous . \n",
      "\n",
      "\n",
      "원문 :  i love italian food . \n",
      "번역문 : j adore la nourriture italienne . \n",
      "예측문 :  j adore le air . \n",
      "\n",
      "\n",
      "원문 :  i can t let you . \n",
      "번역문 : je ne peux pas vous le permettre . \n",
      "예측문 :  je ne vous pas pas ? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 216066,
     "status": "ok",
     "timestamp": 1600594811505,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "lmVTBcoUSXjt",
    "outputId": "25827e36-251f-4e36-f511-725cd28c18b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  i ll take a shower . \n",
      "번역문 : je vais prendre une douche . \n",
      "예측문 :  je veux un train de a un maison . \n",
      "\n",
      "\n",
      "원문 :  i m optimistic . \n",
      "번역문 : je suis optimiste . \n",
      "예측문 :  je suis . \n",
      "\n",
      "\n",
      "원문 :  i feel wonderful . \n",
      "번역문 : je me sens a merveille . \n",
      "예측문 :  je ne suis pas . \n",
      "\n",
      "\n",
      "원문 :  it s time to do it . \n",
      "번역문 : il est temps de le faire . \n",
      "예측문 :  c est un un un voiture . \n",
      "\n",
      "\n",
      "원문 :  taxes are too high . \n",
      "번역문 : les impots sont trop eleves . \n",
      "예측문 :  les sont sont sont . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_test[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_test[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 216066,
     "status": "ok",
     "timestamp": 1600594811506,
     "user": {
      "displayName": "고병화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLZcAZyPt33HiIqtXjHiL8D4PIcVPdfLgODP1QBA=s64",
      "userId": "12645007744594631320"
     },
     "user_tz": -540
    },
    "id": "5ls7-3rySXjv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07_기계번역.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
