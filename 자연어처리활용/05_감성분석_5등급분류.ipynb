{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감성분석 예제  2: 영화 감상평  예제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f180c750a0d060cfc94e52c3771e6d67644913f"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "# import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f03ae0af5d73a29039f39632d1051ab6de4f3fb"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"train.tsv\", sep=\"\\t\")  # tsv: tab seperated value\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb1464a9006164fe78aa10d694328dc24159394c"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47d8b13e01fa17aae32be74d598939403b2cfebb"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e1fdc5dbea7aea92a2872c746aea9b25788aad5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "\n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        \n",
    "        #remove html content\n",
    "        # review_text = BeautifulSoup(sent).get_text()\n",
    "        \n",
    "        #remove non-alphabetic characters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", sent)\n",
    "    \n",
    "        #tokenize the sentences\n",
    "        words = word_tokenize(review_text.lower())\n",
    "    \n",
    "        #lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "        reviews.append(lemma_words)\n",
    "\n",
    "    return(reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sentences = clean_sentences(train)\n",
    "# test_sentences = clean_sentences(test)\n",
    "print(len(train_sentences))\n",
    "# print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31e462eadcf747d4f37a5e6c2bc4965d39ce10bb"
   },
   "outputs": [],
   "source": [
    "target=train.Sentiment.values\n",
    "print(target,target.shape)       # (156060,)\n",
    "y_target=to_categorical(target)  # one-hot 벡터로 변환\n",
    "print(y_target,y_target.shape)   # (156060, 5)\n",
    "num_classes=y_target.shape[1]\n",
    "print('num_classes:',num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16fbf94281d219307165e3195eb799360cc40a53"
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33db4ea234f1f76a98f2c0dbe7d9b9938a1fe2d3"
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef359e011decf1ed1f285470d9b3fa1411d55e35"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c634680111a84642b151e862040b0475520d5010"
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "for sent in tqdm(X_train):\n",
    "    \n",
    "    unique_words.update(sent)   # 집합에 넣어 중복 삭제\n",
    "    \n",
    "    if(len_max<len(sent)):  \n",
    "        len_max = len(sent)  # 가장 긴 문장의 길이를 구한다\n",
    "        \n",
    "#\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(unique_words)[-1] # 마지막 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7dfc49aeb99251a822a39fed9a8fae0171deb662"
   },
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f106b6e59545f8d0d4db272acf32b2337e1ad7b"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "#texts_to_sequences(texts)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "# X_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "print(X_train.shape,X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f99132a978d0cfa245defb35edfbfc5e8f6151c6"
   },
   "source": [
    "## 학습 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27f0a1c26bee93cd660a69df8b79c61094f61abf"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(len(list(unique_words)),300,input_length=len_max),\n",
    "    LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True),\n",
    "    LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False),\n",
    "    Dense(100,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes,activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b1bf946e278f0a9955356351907d512f5018582"
   },
   "outputs": [],
   "source": [
    "\n",
    "history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=3, batch_size=256, verbose=1,\n",
    "                  callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1524533e318bcdea0e4d3abebbf5b02c6eb49b2b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_count = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.plot(epoch_count, history.history['loss'], 'r--')\n",
    "plt.plot(epoch_count, history.history['val_loss'], 'b-')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9356081721990b2af9de625c7fcad7c241ee61a"
   },
   "source": [
    "### 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
