{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq : Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '6': 1, '+': 2, '7': 3, '5': 4, ' ': 5, '_': 6, '9': 7, '2': 8, '0': 9, '3': 10, '8': 11, '4': 12}\n",
      "{0: '1', 1: '6', 2: '+', 3: '7', 4: '5', 5: ' ', 6: '_', 7: '9', 8: '2', 9: '0', 10: '3', 11: '8', 12: '4'}\n",
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n",
      "71+118 \n",
      "_189 \n",
      "510+223\n",
      "_733 \n"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "\n",
    "# 'addition.txt' : 총 50000개 덧셈 연산 예를 가짐\n",
    "\n",
    "# 덧셈식을 일반 문장과 같이 corpus를 생성하고 seed가 고정된 랜덤으로 뒤섞고 90:10 비율로 학습과 검증 데이터를 분리해준다\n",
    "# x는 덧셈식, t는 덧셈 결과값\n",
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('addition.txt',seed=1984)\n",
    "\n",
    "char_to_id ,id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(char_to_id)\n",
    "print(id_to_char)\n",
    "# 총 13개의 문자를 value로 갖음: '0','1','2','3','4','5','6','7','8','9','+',' ','_'\n",
    "\n",
    "print(x_train.shape,t_train.shape)  # (45000, 7) (45000, 5)\n",
    "print(x_test.shape,t_test.shape)    # (5000, 7) (5000, 5)\n",
    "\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n",
    "\n",
    "print(''.join(id_to_char[c] for c in x_train[0])) # 71+118 \n",
    "print(''.join(id_to_char[c] for c in t_train[0])) # _189\n",
    "\n",
    "print(''.join(id_to_char[c] for c in x_train[1]))   \n",
    "print(''.join(id_to_char[c] for c in t_train[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PPL 계산 함수\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters) \n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 머릿글자\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    # 문자열로 변환\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}  # https://norux.me/29 , 이스케이프 문자\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder와 Decoder\n",
    "seq2seq 를 Encoder Decoder 모델이라고도 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoder:\n",
    "    def __init__(self,vocab_size,wordvec_size,hidden_size):\n",
    "        V,D,H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V,D)/100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b  = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "        \n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "        \n",
    "    def forward(self,xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:,-1,:] #  마지막 TimeLSTM 계층의 은닉 상태를 반환\n",
    "    \n",
    "    def backward(self,dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:,-1,:] = dh\n",
    "        \n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder:\n",
    "    def __init__(self,vocab_size,wordvec_size,hidden_size):\n",
    "        V,D,H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V,D)/100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b  = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        affine_W = (rn(H,V) / np.sqrt(H)).astype('f')   # \n",
    "        affine_b  = np.zeros(V).astype('f')             #\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W,affine_b)     #\n",
    "        \n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in (self.embed,self.lstm,self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    # 학습시 호출\n",
    "    def forward(self,xs,h):\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score # softmax를 통과시키지 않고 그냥 출력\n",
    "    \n",
    "    def backward(self,dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        \n",
    "        dh = self.lstm.dh  # TimeLSTM의 backward()에서 dh가 얻어져 저장 되어 있으므로\n",
    "        return dh          # Encoder에 전달 \n",
    "    \n",
    "    # 문장 생성시 호출\n",
    "    def generate(self, h, start_id, sample_size): # h는 은닉상태\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            sample_id = np.argmax(score.flatten()) # 점수가 가장 큰 문자의 ID를 선택,결정적 방법\n",
    "            sampled.append(int(sample_id))\n",
    "            \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq():\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
