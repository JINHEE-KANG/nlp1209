{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 사용 문장 생성 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn_layers import softmax,TimeDropout,Rnnlm,BetterRnnlm,RnnlmTrainer\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm):  # Rnnlm class를 상속 받아 사용\n",
    "    def generate(self,startd_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]  # start_id : 최초로 시작할 단어\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "            # print('p.shape:',p.shape)     # (10000,)\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p) \n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))  # # word_ids 리스트에 샘플링된 단어를 추가\n",
    "                \n",
    "        return word_ids\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.lstm_layer.h, self.lstm_layer.c\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.lstm_layer.set_state(*state)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BetterRnnlmGen(BetterRnnlm) \n",
    "# class BetterRnnlmGen(BetterRnnlm):  # BetterRnnlm class를 상속 받아 사용\n",
    "#     def generate(self,startd_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "#         word_ids = [start_id]  # start_id : 최초로 시작할 단어\n",
    "        \n",
    "#         x = start_id\n",
    "#         while len(word_ids) < sample_size:\n",
    "#             x = np.array(x).reshape(1,1)\n",
    "#             score = self.predict(x)\n",
    "#             p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "#             # print('p.shape:',p.shape)     # (10000,)\n",
    "            \n",
    "#             sampled = np.random.choice(len(p),size=1,p=p) \n",
    "#             # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "#             if (skip_ids is None) or (sampled not in skip_ids):\n",
    "#                 x = sampled\n",
    "#                 word_ids.append(int(x))  # # word_ids 리스트에 샘플링된 단어를 추가\n",
    "                \n",
    "#         return word_ids\n",
    "    \n",
    "#     def get_state(self):\n",
    "#         states = []\n",
    "#         for layer in self.lstm_layers:\n",
    "#             states.append((layer.h, layer.c))\n",
    "#         return states\n",
    "\n",
    "#     def set_state(self, states):\n",
    "#         for layer, state in zip(self.lstm_layers, states):\n",
    "#             layer.set_state(*state)    \n",
    "\n",
    "class BetterRnnlmGen(BetterRnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x).flatten()\n",
    "            p = softmax(score).flatten()\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장생성을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you recycled hope recommends obtained mortgage-backed smith adjust endorsed deficit artistic lortie unchanged deprived kidder more uncommon boiler compensate territory haven opinions whitbread solely taxable interviewed uncertainty fill exhibition katz replace travel bit singer wanted ends inquiries hook heads challenge frederick fisher gate portrayal video cameras fur chancellor acknowledged expenditure carry bullish establish venice client parental meant wines owen easy choose inner-city duff mart chambers purchasing interviews march success defined double-a nippon ounces drifted toseland andrew achievement continent user joel represent plate kageyama milwaukee disappeared whites killed insurance computer gatt american ivory convenience others youth mips repeat jurors considers biggest\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('Rnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 좋은 문장으로 : 2층 LSTM,  Dropout, 가중치 공유 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you care hastings fourth-quarter survived lorin bernstein unveiled resident windows portion longstanding toseland alternatives desirable seattle salomon aeronautics n.h. disclosures labor-management divisions usa mode digs notes nasd winnebago zero management-led parent much defective cohen expect pretty accused dealership century lowe eagle single-a-3 sunnyvale assist arco romantic hollywood colgate creditors appointments affluent type aspirations instead abrams dorrance tim combat types noticed colo. lowe initiated tandy clifford improves jones coordinate dignity a.m. satisfied war communication scrambling households venezuela pinnacle poughkeepsie lynn discretionary demler barometer consistently standardized withstand disappeared smallest streak technicians asserts usual theatrical closed giving ortega lbo registered pound properly sheets\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params('BetterRnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어열을 초기 값으로 주고 문장을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the meaning of life is soo kobe preventing repayment locally roebuck backers pbs consists advertising speaker sells stocks sentence agricultural vinson commercial demanding diamond undermine hub pesticides streamline settlements contemplating areas sporadic affidavits create corsica wild two-tier motors someday ltv clarify rebounding overall example horrible marlowe invented refineries generous enron youth highway memphis b susceptible friendship resolve violating satisfy terry accounted peter blocking affect managua home leveraged o. oat strip software magnitude plc expand inspectors chinese integrity shuttle coleman remedy explicit sporting parent general speaking foreseeable peck devices dax factories starting russell tax-free restructurings earth he guarantees assume century barriers motion reward march assuming\n"
     ]
    }
   ],
   "source": [
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)  # 마지막 단어('is')를 시작 단어로 문장 생성\n",
    "word_ids = start_ids[:-1] + word_ids                # 'is' 앞까지의 단어를 앞부분에 추가\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)  #  실행시 마다 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsequent\n",
      "plants\n",
      "indicates\n",
      "thrift\n"
     ]
    }
   ],
   "source": [
    "# 'the meaning of life' 부분 예측  :  'meaning of life is' 으로 예측 되지 않음\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    score = model.predict(x).flatten()\n",
    "    p = softmax(score).flatten()\n",
    "    sampled = np.random.choice(len(p), size=1, p=p)\n",
    "    print(id_to_word[sampled[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
