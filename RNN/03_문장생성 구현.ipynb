{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 사용 문장 생성 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn_layers import softmax,TimeDropout,Rnnlm,BetterRnnlm,RnnlmTrainer\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm):  # Rnnlm class를 상속 받아 사용\n",
    "    def generate(self,startd_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]  # start_id : 최초로 시작할 단어\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "            # print('p.shape:',p.shape)     # (10000,)\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p) \n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))  # # word_ids 리스트에 샘플링된 단어를 추가\n",
    "                \n",
    "        return word_ids\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.lstm_layer.h, self.lstm_layer.c\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.lstm_layer.set_state(*state)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BetterRnnlmGen(BetterRnnlm) \n",
    "class BetterRnnlmGen(BetterRnnlm):  # BetterRnnlm class를 상속 받아 사용\n",
    "    def generate(self,startd_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]  # start_id : 최초로 시작할 단어\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "            # print('p.shape:',p.shape)     # (10000,)\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p) \n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))  # # word_ids 리스트에 샘플링된 단어를 추가\n",
    "                \n",
    "        return word_ids\n",
    "    \n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장생성을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you survey clarify funded pet stable indian fly adoption virtue variety volatility newest webster neatly consultant ohio ringer doubled microprocessors sympathetic kravis discourage haas republic kobe globe printer clearly mateo data frustration promise becoming hut smallest young launching pile kind primarily clearance hostile embarrassed recapitalization metal gotten antar underwriters cautiously disagreed born premises spends egg anymore mean science asset protects stuck ministries illinois college clifford two-tier residence background niche bracing alan injuries thomas october influential beretta disciplined buffet mitchell bare-faced extreme tapes convincing statewide excuse ambrosiano economics version architect risky probably investigating garrison poughkeepsie 300-a-share honesty imagine bank-backed successfully logic\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('Rnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 좋은 문장으로 : 2층 LSTM,  Dropout, 가중치 공유 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you benjamin division guaranteed transfers graduates hoping predicted diplomat charles subsidiary classroom mail hees aids medication activities in-house aging n. donating alert frustration winnebago master unable laying gross hart pride improve ninth for simmons vs. declining helpful blind throwing considers folks batch leaped ethical convince morristown etc 20th alternative dig precisely row ogden judge know-how ge ' yen tiny predict corp. sued agreement core newsletters state-controlled bellwether fixed-rate folk girlfriend delivering leader voiced write-offs award developing translated proper shaping tire built criteria sharply capacity balls bribe why restructured further advancing seems laws cineplex grossly players bunny systems full sanctions memory\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params('BetterRnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어열을 초기 값으로 주고 문장을 생성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
